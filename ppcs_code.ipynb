{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import neurokit2 as nk\n",
    "import hrvanalysis as hrv\n",
    "from scipy import ndimage\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and feature extraction\n",
    "\n",
    "folder_path = '/home/mw/input/hfindex7735/vol1/PPC'\n",
    "npz_files = [os.path.join(root, file) for root, _, files in os.walk(folder_path) for file in files if file.endswith('.npz')]\n",
    "ecg = []\n",
    "rsp_ch = []\n",
    "rsp_ab = []\n",
    "spo = []\n",
    "acc = []\n",
    "for i, file_path in enumerate(npz_files):\n",
    "    data = np.load(file_path)\n",
    "    ecg.append(data['ecg_list'])\n",
    "    rsp_ch.append(data['rsp_ch_list'])\n",
    "    rsp_ab.append(data['rsp_ab_list'])\n",
    "    spo.append(data['spo_value'])\n",
    "    acc.append(data['acc'])\n",
    "\n",
    "\n",
    "ecg_raw = ecg\n",
    "rsp_ch_raw = rsp_ch\n",
    "rsp_ab_raw = rsp_ab\n",
    "spo_raw = spo\n",
    "acc_raw = acc\n",
    "\n",
    "def smooth_signal(signal, size):\n",
    "    smoothed_signal = np.zeros(len(signal))\n",
    "    for i in np.arange(len(signal)):\n",
    "        if i == 0:\n",
    "            smoothed_signal[i] = signal[i]\n",
    "        elif i < np.divide((size - 1), 2):\n",
    "            smoothed_signal[i] = np.mean(signal[:i * 2 + 1])\n",
    "        elif i > len(signal) - np.divide((size - 1), 2) - 1:\n",
    "            smoothed_signal[i] = np.mean(signal[i - (len(signal) - 1 - i):len(signal)])\n",
    "        else:\n",
    "            start = int(i - np.divide((size - 1), 2))\n",
    "            end = int(i + 1 + np.divide((size - 1), 2))\n",
    "            smoothed_signal[i] = np.mean(signal[start:end])\n",
    "    return smoothed_signal\n",
    "\n",
    "\n",
    "def remove_outliers(signal, size):\n",
    "\n",
    "    for i in np.arange(size, len(signal) - size):\n",
    "        mean = np.mean(np.hstack((signal[i-size:i],signal[i+1:i+size+1])))\n",
    "        std = np.std(np.hstack((signal[i-size:i],signal[i+1:i+size+1])))\n",
    "        if ((signal[i] > mean + 3*std) | (signal[i] < mean - 3*std)):\n",
    "            signal[i] = mean\n",
    "    return signal\n",
    "\n",
    "\n",
    "def spo_process(spo2_values):\n",
    "    spo2_values = np.array(spo2_values)\n",
    "    spo2_values = spo2_values[spo2_values != 0]\n",
    "    \n",
    "    percent_above_95 = np.sum(spo2_values > 95) / len(spo2_values) * 100\n",
    "    percent_above_90 = np.sum(spo2_values > 90) / len(spo2_values) * 100\n",
    "    percent_above_85 = np.sum(spo2_values > 85) / len(spo2_values) * 100\n",
    "    percent_above_80 = np.sum(spo2_values > 80) / len(spo2_values) * 100\n",
    "    \n",
    "    return {\n",
    "        \"percent_above_95\": percent_above_95,\n",
    "        \"percent_above_90\": percent_above_90,\n",
    "        \"percent_above_85\": percent_above_85,\n",
    "        \"percent_above_80\": percent_above_80\n",
    "    }\n",
    "\n",
    "def rr_process(ecg, ecg_peaks, th=10):\n",
    "    \"\"\"\n",
    "    Calculate the arrhythmic load.\n",
    "    :param ecg: An array of electrocardiogram signals\n",
    "    :param ecg_peaks: Indices of R-wave positions\n",
    "    :param th: Threshold for heart rate variation, default is 10 beats per minute\n",
    "    :return: Arrhythmic load (number of arrhythmias per total time)\n",
    "    \"\"\"\n",
    "    ect_r_ind = []  # List to store arrhythmic positions\n",
    "    \n",
    "    total_time_seconds = len(ecg) / 200\n",
    "    total_time = total_time_seconds / 60\n",
    "    \n",
    "    rr_intervals = np.diff(ecg_peaks) / 200\n",
    "    hr_per_peak = 60 / rr_intervals\n",
    "    \n",
    "    hr_per_peak = hr_per_peak.tolist()  # Convert heart rate list to standard list format\n",
    "    ecg_peaks = ecg_peaks[1:].tolist()  # Remove the first R-wave since it has no previous comparison\n",
    "    et_count = 0  # Arrhythmia count\n",
    "    i = 1  # Start checking from the second position\n",
    "\n",
    "    # Check heart rate differences before and after each R-wave position\n",
    "    while i < len(hr_per_peak) - 2:\n",
    "        # If the heart rate variation exceeds the threshold, consider it an arrhythmia\n",
    "        if (abs(hr_per_peak[i] - hr_per_peak[i - 1]) >= th and abs(hr_per_peak[i] - hr_per_peak[i + 1]) >= th) or \\\n",
    "           abs(hr_per_peak[i + 1] - hr_per_peak[i + 2]) >= th:\n",
    "            if abs(hr_per_peak[i] - hr_per_peak[i - 1]) < th:\n",
    "                i += 1\n",
    "            else:\n",
    "                hr_per_peak.pop(i)\n",
    "                ecg_peaks.pop(i)\n",
    "                ect_r_ind.append(ecg_peaks[i])  # Record the arrhythmic position\n",
    "                et_count += 1  # Increase the arrhythmia count\n",
    "        else:\n",
    "            i += 1  # If no arrhythmia, continue to the next position\n",
    "\n",
    "    # Calculate arrhythmic load\n",
    "    ectopic_load = et_count / total_time if total_time > 0 else 0  # Avoid division by zero\n",
    "\n",
    "    return ectopic_load\n",
    "\n",
    "\n",
    "def rsp_process(rsp_info,respList):\n",
    "    rip_peaks = rsp_info['RSP_Peaks']\n",
    "    rip_troughs = rsp_info['RSP_Troughs']\n",
    "    if rip_peaks[0] < rip_troughs[0]:\n",
    "        rip_peaks = np.delete(rip_peaks, 0)\n",
    "    if rip_peaks[-1] > rip_troughs[-1]:\n",
    "        rip_peaks = np.delete(rip_peaks, -1)\n",
    "    resp_feature = dict()\n",
    "    br = 60/(np.diff(rip_troughs)/ 25)\n",
    "    br_index = np.arange(len(br))\n",
    "    resp_feature['br_mean'] = np.mean(br[br_index])\n",
    "    resp_feature['br_max'] = np.max(br[br_index])\n",
    "    resp_feature['br_min'] = np.min(br[br_index])\n",
    "    resp_feature['br_cv'] = np.std(br[br_index]) / np.mean(br[br_index]) * 100\n",
    "    \n",
    "    TI_interval, TE_interval, VT_in, VT_ex = [], [], [], []\n",
    "\n",
    "    for i in range(len(rip_peaks)):\n",
    "        temp = rip_troughs[i + 1] - rip_peaks[i]  # huqi\n",
    "        temp2 = rip_peaks[i] - rip_troughs[i]  # xiqi\n",
    "        vt_ex_temp = respList[rip_peaks[i]] - respList[rip_troughs[i + 1]]\n",
    "        vt_in_temp = respList[rip_peaks[i]] - respList[rip_troughs[i]]\n",
    "        TI_interval.append(temp2)\n",
    "        TE_interval.append(temp)\n",
    "        VT_ex.append(vt_ex_temp)\n",
    "        VT_in.append(vt_in_temp)\n",
    "\n",
    "    TI_interval = np.array(TI_interval) / 25\n",
    "    TE_interval = np.array(TE_interval) / 25\n",
    "    TI_ratio = TI_interval / (TI_interval + TE_interval)\n",
    "    resp_feature['TI_mean'] = np.mean(TI_interval[br_index])\n",
    "    resp_feature['TI_std'] = np.std(TI_interval[br_index])\n",
    "    resp_feature['TI_cv'] = (np.std(TI_interval[br_index])) / (np.mean(TI_interval[br_index])) * 100\n",
    "    resp_feature['TE_mean'] = np.mean(TE_interval[br_index])\n",
    "    resp_feature['TE_std'] = np.std(TE_interval[br_index])\n",
    "    resp_feature['TE_cv'] = (np.std(TE_interval[br_index])) / (np.mean(TE_interval[br_index])) * 100\n",
    "    resp_feature['TI_ratio_mean'] = np.mean(TI_ratio[br_index])\n",
    "    resp_feature['TI_ratio_std'] = np.std(TI_ratio[br_index])\n",
    "    resp_feature['TI_ratio_cv'] = (np.std(TI_ratio[br_index])) / (np.mean(TI_ratio[br_index])) * 100\n",
    "   \n",
    "    min_ven_in = br * VT_in\n",
    "    min_ven_ex = br * VT_ex\n",
    "    \n",
    "    resp_feature['min_ven_in_mean'] = np.mean(min_ven_in[br_index])\n",
    "    resp_feature['min_ven_in_std'] = np.std(min_ven_in[br_index])\n",
    "    resp_feature['min_ven_in_cv'] = (np.std(min_ven_in[br_index])) / (np.mean(min_ven_in[br_index])) * 100\n",
    "    resp_feature['min_ven_ex_mean'] = np.mean(min_ven_ex[br_index])\n",
    "    resp_feature['min_ven_ex_std'] = np.std(min_ven_ex[br_index])\n",
    "    resp_feature['min_ven_ex_cv'] = (np.std(min_ven_ex[br_index])) / (np.mean(min_ven_ex[br_index])) * 100\n",
    "    \n",
    "    RSBI_in = br/ VT_in\n",
    "    RSBI_ex = br/ VT_in\n",
    "    \n",
    "    resp_feature['RSBI_in_mean'] = np.mean(RSBI_in[br_index])\n",
    "    resp_feature['RSBI_in_std'] = np.std(RSBI_in[br_index])\n",
    "    resp_feature['RSBI_in_cv'] = (np.std(RSBI_in[br_index])) / (np.mean(RSBI_in[br_index])) * 100\n",
    "    resp_feature['RSBI_ex_mean'] = np.mean(RSBI_ex[br_index])\n",
    "    resp_feature['RSBI_ex_std'] = np.std(RSBI_ex[br_index])\n",
    "    resp_feature['RSBI_ex_cv'] = (np.std(RSBI_ex[br_index])) / (np.mean(RSBI_ex[br_index])) * 100\n",
    "    \n",
    "    return rsp_feature\n",
    "\n",
    "\n",
    "def SleepApneaDetector():\n",
    "    \"\"\"\n",
    "    Pseudocode for SleepApneaDetector:\n",
    "\tInput:\n",
    "\t    rsp_ch: An array of chest respiratory signals \n",
    "\t    rsp_ab: An array of abdominal respiratory signals \n",
    "\t    spo: An array of blood oxygen saturation signals\n",
    "\t    acc: An array of three-axis acceleration signals\n",
    "\t    \n",
    "\tOutput:\n",
    "\t    ahi: Apnea-Hypopnea Index (AHI)\n",
    "\n",
    "\tProcedure:\n",
    "    ### 1. Data Preparation\n",
    "    1. **Input Data**: Obtain respiratory signals, blood oxygen saturation data, and triaxial acceleration data. \n",
    "    2. **Initialize Parameters**: Set the calculation parameters for the apnea threshold (TH_A) and hypopnea threshold (TH_H) (e.g., TH_A = AMP_MAX × 0.35, TH_H = AMP_MAX × 0.7, with AMP_MAX initially set to 0) for subsequent threshold calculations; set the time window for AHI calculation (e.g., 1 hour, adjustable according to actual needs) and event counters (e.g., apnea_count = 0, hypopnea_count = 0, used to record the number of apnea and hypopnea events respectively).\n",
    "    \n",
    "    ### 2. Tidal Volume Calculation and Preprocessing\n",
    "    1. Calculate the tidal volume (VT) using the formula \\(VT = K × RC + M × AB\\) based on the preprocessed chest and abdominal respiratory signals, where K is the chest respiratory signal fitting coefficient, M is the abdominal respiratory signal fitting coefficient, RC is the preprocessed chest respiratory signal, and AB is the preprocessed abdominal respiratory signal.\n",
    "    2. Filter and standardize the calculated tidal volume.\n",
    "    \n",
    "    ### 3. Calculate Respiratory Amplitude and Respiratory Amplitude Baseline\n",
    "    1. Detect the peaks and valleys of the tidal volume to calculate the respiratory amplitude (AMP) sequence (the amplitude of the peak signal minus the amplitude of the adjacent valley signal). If \\(AMP ≤ 0.15\\), it is considered a false peak and removed.\n",
    "    2. Calculate the respiratory amplitude baseline (AMP_MAX) by calculating the obtained AMP.\n",
    "    3. Interpolate the respiratory amplitude and respiratory amplitude baseline according to the sampling frequency of the chest and abdominal respiratory signals to make them correspond to the respiratory signal.\n",
    "    \n",
    "    ### 4. Calculate Apnea and Hypopnea Thresholds\n",
    "    1. Calculate the apnea threshold (TH_A) using the formula \\(TH_A = AMP_MAX × 0.35\\) based on the currently calculated AMP_MAX.\n",
    "    2. Calculate the hypopnea threshold (TH_H) using the formula \\(TH_H = AMP_MAX × 0.7\\).\n",
    "    \n",
    "    ### 5. Preliminary Determination of Sleep Respiratory Events\n",
    "    1. Based on the detection results of the tidal volume peaks and valleys, preliminarily judge the sleep respiratory events according to the following criteria:\n",
    "        - **Central Sleep Apnea Judgment**:\n",
    "            - If the interval between two adjacent peaks of the respiratory amplitude (AMP) curve is greater than 10 seconds, and there is no respiratory effort signal in both chest and abdominal respirations, it is judged as central sleep apnea, and apnea_count is incremented by 1.\n",
    "            - If the AMP is less than TH_A for more than 10 seconds, and no peak is detected, or the peak interval exceeds 10 seconds, it is judged as central sleep apnea, and apnea_count is incremented by 1.\n",
    "        - **Obstructive Sleep Apnea Judgment**:\n",
    "            - If the interval between two adjacent peaks of the respiratory amplitude (AMP) curve is greater than 10 seconds, and at least one of the chest and abdominal respiratory signals shows respiratory effort, it is judged as obstructive sleep apnea, and apnea_count is incremented by 1.\n",
    "            - If the AMP is less than TH_A for more than 10 seconds, and more than 6 peaks are detected, or the detected peaks are no more than 6 and the peak interval does not exceed 10 seconds, it is judged as obstructive sleep apnea, and apnea_count is incremented by 1.\n",
    "        - **Mixed Sleep Apnea Judgment**: If the same time period simultaneously meets the judgment criteria of central sleep apnea and obstructive sleep apnea, and the central apnea occurs before the obstructive apnea, it is judged as mixed sleep apnea, and apnea_count is incremented by 1 (here, it can be considered whether to count the mixed sleep apnea separately or include it in the total number of apnea events according to specific requirements. Assume it is included in apnea_count).\n",
    "        - **Hypopnea Judgment**: If the AMP is less than TH_H but greater than TH_A for more than 10 seconds, and accompanied by a decrease in blood oxygen saturation of more than 4%, it is judged as hypopnea, and hypopnea_count is incremented by 1.\n",
    "    2. For each detected event, record its occurrence time (start_time) and duration (duration).\n",
    "    \n",
    "    ### 6. Calibrate Sleep Respiratory Events\n",
    "    1. **Delete False Positives Caused by Artifacts**:\n",
    "        - Calculate the signal quality index, which is the variance of the respiratory amplitude change (VAR(diff(AMP))), with the variance window set to 10 breaths. If there is a triaxial acceleration signal, the threshold is set to 4 times AMP_MAX; if there is no triaxial acceleration signal, the threshold is set to 2 times AMP_MAX. If the signal quality is poor, remove the segment of the signal and the previously detected apnea events within 2 minutes after this segment (reduce apnea_count or hypopnea_count accordingly).\n",
    "        - Calculate the mean respiratory amplitude Mean(AMP) of each apnea and hypopnea event, and the mean respiratory amplitude Mean(AMP_after2min) 2 minutes after the event. If \\(Mean(AMP) >= 0.7 * Mean(AMP_after2min)\\), it is considered interference and the event is removed (reduce the corresponding count).\n",
    "    2. **Calibrate with Blood Oxygen Signal**:\n",
    "        - Calibrate the results after the previous step with the blood oxygen signal. If the blood oxygen does not decrease by 4% or more after a hypopnea event, remove the hypopnea event (hypopnea_count is decremented by 1); if the blood oxygen does not decrease by 3% or more after an apnea event, remove the apnea event (apnea_count is decremented by 1). During calibration, record the position of blood oxygen loss. If there is a loss of blood oxygen for more than 10 seconds when a respiratory event occurs, retain the event.\n",
    "    3. **Calibrate with Triaxial Acceleration Signal**: Use the triaxial acceleration signal to identify the subject's sleeping position and activity state, and remove the apnea events detected during non-lying and active states and within 1 minute after that (reduce apnea_count accordingly).\n",
    "    \n",
    "    ### 7. Calculate AHI\n",
    "    1. When the set time window (e.g., 1 hour) is reached, calculate the AHI using the formula \\(AHI = (apnea_count + hypopnea_count) / time window (in hours)\\) based on the current number of apnea events (apnea_count) and hypopnea events (hypopnea_count).\n",
    "    2. Output the calculated AHI value for evaluating the severity of the subject's sleep apnea - hypopnea syndrome.\n",
    "    \n",
    "    ### 8. Loop Processing\n",
    "    1. Slide the time window forward (e.g., slide 1 minute, adjustable according to actual needs), update the data range, and repeat steps 3 - 8 to continuously calculate the AHI value throughout the sleep monitoring period.\n",
    "    \n",
    "    Please note that this section pertains to the patented invention \"Sleep Apnoea Event Detection Method\" (ZL 202110167676.X), which was granted to the Chinese PLA General Hospital on 4 July, 2023. \n",
    "    \"\"\"\n",
    "    return ahi\n",
    "\n",
    "def StageClassifier():\n",
    "    \"\"\"\n",
    "    Pseudocode for StageClassifier:\n",
    "\tInput:\n",
    "\t    ecg: An array of electrocardiogram signals\n",
    "\t    rsp: An array of respiratory signals \n",
    "\t    \n",
    "\tOutput:\n",
    "\t    predict_result: NREM1(Non-Rapid Eye Movement1 sleep periods); NREM2_per,(Non-Rapid Eye Movement2 sleep periods) ; NREM3_per(Non-Rapid Eye Movement3 sleep periods); REM_per, (Rapid Eye Movement sleep periods)\n",
    "\n",
    "\tProcedure:    \n",
    "    ### 1. RR Interval Calculation and Processing\n",
    "    1. **Calculate RR Interval Sequence**\n",
    "       - Calculate the time interval between adjacent R-waves based on the detected R-wave peaks to obtain the RR interval sequence (RRn).\n",
    "    2. **Data Cleaning and Interpolation**\n",
    "       - Check for abnormal values in the RR interval sequence (such as RR intervals that are too long or too short and outside normal physiological range). Replace the abnormal values with the average of adjacent RR intervals.\n",
    "    \n",
    "    ### 2. Feature Extraction\n",
    "    1. **Extract Features from RR Interval Signal **\n",
    "       - Calculate the mean of RR intervals (Mean_RR),the standard deviation of RR intervals (SDNN),the root mean square of the differences between adjacent RR intervals (RMSSD),the percentage of adjacent RR intervals with a difference greater than 50ms (pNN50),frequency domain features such as low frequency (LF), medium frequency (MF), high frequency (HF), and total power (TF) (by performing power spectral analysis on the RR interval sequence, using the Fast Fourier Transform (FFT)), and calculate their normalized values (such as LFn, MFn, HFn, etc.) and the ratios between different frequency components (such as LF/HF, etc.)\n",
    "    2. **Extract Features from Respiratory Signal**\n",
    "       - Calculate the inhalation time, exhalation time, and their related statistics (such as mean, standard deviation, etc.) of the breathing signal\n",
    "       - Calculate the peak and trough related features of the breathing signal, such as the median of the time difference between the peak and the trough, the median of the inhalation area, the median of the exhalation area, and the median of the single breath area\n",
    "       - Calculate the power spectral density of the breathing signal to obtain features such as the energy of high frequency, low frequency, very low frequency bands, and the ratio of high frequency to total frequency\n",
    "        \n",
    "    3. **Extract Features through Cardiorespiratory Coupling**\n",
    "       - Calculate the cardiorespiratory coupling (CPC) index: Calculate the CPC index through a specific algorithm (such as based on the synchronous analysis of ECG and breathing signals), and obtain features such as the ratio of the sum of the CPC index in different frequency bands to the sum of the CPC index in the entire frequency band (such as high frequency/total power, low frequency/total power, very low frequency/total power, low frequency/high frequency, etc.)\n",
    "       - Calculate the statistical features related to the CPC index, such as the standard deviation of the absolute value of the difference between the left and right halves\n",
    "       \n",
    "    ### 3. Sleep Stage Prediction\n",
    "    1. **Input Features into the Model** (an existing trained BLSTM model.The accuracy and reliability of the model has been verified.(Wang, Z., et al. Development and Validation of Algorithms for Sleep Stage Classification and Sleep Apnea/Hypopnea Event Detection Using a Medical-Grade Wearable Physiological Monitoring System. in International Conference on Wireless Mobile Communication and Healthcare. 2021. Springer.))\n",
    "       - Compose the extracted features into a feature vector and input it into the trained BLSTM model in the format required by the model.\n",
    "    2. **Model Prediction**\n",
    "       - The model predicts based on the input vector and outputs the probability values of each sleep stage category (such as NREM1, NREM2_per, NREM3_per, REM_per) corresponding to each time point.\n",
    "    3. **Determine the Sleep Stage Result**\n",
    "       - For each time point, select the category with the highest probability as the sleep stage result based on the output probability values of the model. For example, if the model predicts the sleep stage category probabilities at a certain time point as [0.1, 0.3, 0.4, 0.2] (corresponding to NREM1, NREM2_per, NREM3_per, REM_per respectively), then the sleep stage result at this time point is NREM3_per.\n",
    "\t\"\"\"\n",
    "    return Sleep_Stage_result\n",
    "\n",
    "def stat_sleep(stages):\n",
    "    sl_re = dict()\n",
    "    wake_time = np.sum(stages == 1)\n",
    "    light_time = np.sum(stages == 2)\n",
    "    deep_time = np.sum(stages == 3)\n",
    "    rem_time = np.sum(stages == 4)\n",
    "    sl_re['tol_len'] = len(stages)*30 / 3600\n",
    "    sl_re['wake_per'] = wake_time / len(stages)*100\n",
    "    sl_re['light_per'] = light_time / len(stages) * 100\n",
    "    sl_re['deep_per'] = deep_time / len(stages) * 100\n",
    "    sl_re['rem_per'] = rem_time / len(stages) * 100\n",
    "    return sl_re\n",
    "\n",
    "def sleep_process(ecg, rsp_ch, rsp_ab, spo, acc):\n",
    "    rsp = rsp_ch + rsp_ab\n",
    "    sleep_stage = StageClassifier(ecg, rsp)\n",
    "    stages = sleep_stage\n",
    "    slp_features = stat_sleep(stages)\n",
    "    \n",
    "    turnover_index = []\n",
    "    lie_list = [2, 3, 4, 5]\n",
    "    for current_pos in np.arange(len(acc.posture) - 2):\n",
    "        if acc.posture[current_pos] in lie_list:\n",
    "            if acc.posture[current_pos + 1] != acc.posture[current_pos] and (\n",
    "                    acc.posture[current_pos + 1] in lie_list):\n",
    "                turnover_index.append(current_pos)\n",
    "    turnover_index = np.array(turnover_index)\n",
    "    diff_turnover = np.diff(turnover_index)\n",
    "    turnover_events_number = len(diff_turnover[diff_turnover >= 60])\n",
    "    slp_features['turn_over'] = turnover_events_number\n",
    "    \n",
    "    ahi, evs = SleepApneaDetector(rsp_ch, rsp_ab, spo, acc)\n",
    "    slp_features['AHI'] = ahi\n",
    "    \n",
    "    return slp_features\n",
    "\n",
    "# smooth:\n",
    "ecg_smoothed = smooth_signal(ecg_raw, size=200)\n",
    "rsp_ch_smoothed = smooth_signal(rsp_ch_raw, size=25)\n",
    "rsp_ab_smoothed = smooth_signal(rsp_ab_raw, size=25)\n",
    "spo_smoothed = smooth_signal(spo_raw, size=10)\n",
    "acc_smoothed = smooth_signal(acc_raw, size=10)\n",
    "\n",
    "#Removing outliers\n",
    "ecg_remove = remove_outliers(ecg_smoothed, size=200)\n",
    "rsp_ch_remove = remove_outliers(rsp_ch_smoothed, size=25)\n",
    "rsp_ab_remove = remove_outliers(rsp_ab_smoothed, size=25)\n",
    "spo_remove = remove_outliers(spo_smoothed, size=10)\n",
    "acc_remove = remove_outliers(acc_smoothed, size=10)\n",
    "\n",
    "_, results = nk.ecg_peaks(ecg_remove, sampling_rate=200, method='Hamilton')\n",
    "nn = results['ECG_R_Peaks']\n",
    "\n",
    "# HRV features\n",
    "hrv_feature = hrv.extract_features.get_time_domain_features(nn_intervals=nn)\n",
    "hrv_feature2 = hrv.extract_features.get_frequency_domain_features(nn_intervals)\n",
    "\n",
    "# Characterization of the arrhythmic load\n",
    "ar_burden = rr_process(ecg_remove,nn)\n",
    "\n",
    "# Respiratory characteristics\n",
    "rsp_remove = rsp_ch_remove + rsp_ab_remove\n",
    "info = {\"RSP_Peaks\": peaks, \"RSP_Troughs\": troughs}\n",
    "rsp_info = nk.rsp_findpeaks(rsp_cleaned=rsp_remove, sampling_rate=25, method=\"khodadad2018\")\n",
    "rsp_feature = rsp_process(rsp_info,rsp_remove)\n",
    "\n",
    "# Oxygen Characteristics\n",
    "spo_feature = spo_process(spo_remove)\n",
    "\n",
    "# Sleep features\n",
    "slp_features = sleep_process(ecg_remove, rsp_ch_remove, rsp_ab_remove, spo_remove, acc_remove)\n",
    "\n",
    "# Merge\n",
    "Physiology_features = hrv_feature.merge(hrv_feature2, on='key', how='inner') \\\n",
    "    .merge(ar_burden, on='key', how='inner') \\\n",
    "    .merge(rsp_feature, on='key', how='inner') \\\n",
    "    .merge(spo_feature, on='key', how='inner') \\\n",
    "    .merge(slp_features, on='key', how='inner')\n",
    "\n",
    "# clinical\n",
    "clinical_features = pd.read_csv('clinical_data.csv')\n",
    "\n",
    "all_features = pd.merge(Physiology_features, clinical_features, on='key', how='inner')\n",
    "\n",
    "with pd.ExcelWriter('Physiological and Clinical Characteristics of Heart valve surgery patients.xlsx') as writer:\n",
    "    clinical_features.to_excel(writer, sheet_name='cln', index=False)\n",
    "    Physiology_features.to_excel(writer, sheet_name='phy', index=False)\n",
    "    all_features.to_excel(writer, sheet_name='both', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature screening\n",
    "\n",
    "excel_file_path = 'Physiological and Clinical Characteristics of Heart valve surgery patients.xlsx'\n",
    "\n",
    "\n",
    "#Choose a different learner\n",
    "models_dict = {\n",
    "    \"XGBoost\": XGBClassifier(learning_rate=0.01, n_estimators=10, max_depth=1, random_state=1),\n",
    "    \"Logistic Regression\": LogisticRegression(C=0.01, random_state=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=10, max_depth=1, random_state=1),\n",
    "    \"SVM\": SVC(kernel='linear', random_state=1, probability=True),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "sheets = ['cln', 'phy', 'both']\n",
    "models = ['XGBoost', 'Logistic Regression', 'Random Forest', 'SVM', 'KNN']\n",
    "\n",
    "#Stored in the corresponding model sheet\n",
    "with pd.ExcelWriter('Feature screened _data.xlsx') as writer:\n",
    "  \n",
    "    for model_name in models:\n",
    "        estimator = models_dict[model_name]  \n",
    "        \n",
    "\n",
    "        for sheet in sheets:\n",
    "            print(f\"Processing data for model , sheet {sheet}\")\n",
    "\n",
    "            df = pd.read_excel(excel_file_path, sheet_name=sheet)\n",
    "            X = df.iloc[:, :-1].values \n",
    "            y = df.iloc[:, -1].values  \n",
    "\n",
    "            rfecv = RFECV(estimator=estimator, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "            rfecv.fit(X, y)\n",
    "\n",
    "            print(f\"Optimal number of features for {model_name} on {sheet}: {rfecv.n_features_}\")\n",
    "            print(f\"Ranking of features for {model_name} on {sheet}: {rfecv.ranking_}\")\n",
    "\n",
    "            selected_features = df.columns[:-1][rfecv.support_]\n",
    "            processed_data = df[selected_features]               \n",
    "            processed_data['Label'] = y                         \n",
    "\n",
    "            output_sheet_name = f\"{model_name}_{sheet}\"\n",
    "            processed_data.to_excel(writer, sheet_name=output_sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "\n",
    "excel_file_path = 'Feature screened _data.xlsx'\n",
    "\n",
    "# Define the model and corresponding sheet names\n",
    "# cln for clinical data, phy for physiologic data, both for total data\n",
    "sheet_mapping = {\n",
    "    \"XGBoost\": [\"xgb_cln\", \"xgb_phy\", \"xgb_both\"],\n",
    "    \"Logistic Regression\": [\"lr_cln\", \"lr_phy\", \"lr_both\"],\n",
    "    \"Random Forest\": [\"rf_cln\", \"rf_phy\", \"rf_both\"],\n",
    "    \"SVM\": [\"svm_cln\", \"svm_phy\", \"svm_both\"],\n",
    "    \"KNN\": [\"knn_cln\", \"knn_phy\", \"knn_both\"]\n",
    "}\n",
    "\n",
    "# Define initial parameters for each model\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBClassifier(learning_rate=0.01, n_estimators=10, max_depth=1, random_state=1),\n",
    "    \"Logistic Regression\": LogisticRegression(C=0.01, random_state=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=10, max_depth=1, random_state=1),\n",
    "    \"SVM\": SVC(kernel='poly', C=0.01, random_state=1, probability=True),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grids = {\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': list(range(10, 101, 10)),\n",
    "        'max_depth': list(range(1, 10)),\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'random_state' : list(range(1,50))\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        'C': [0.01, 0.1, 0.2, 0.5, 1, 10]\n",
    "        'random_state' : list(range(1,50))\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [10, 40, 70, 100],\n",
    "        'max_depth': [1, 2, 4, 6, 8]\n",
    "        'random_state' : list(range(1,50))\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': [0.01, 0.1, 0.2, 0.5, 1],\n",
    "        'kernel': ['linear', 'poly', 'rbf']\n",
    "        'random_state' : list(range(1,50))\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Read and process the corresponding sheet for each model and perform parameter search, training and evaluation\n",
    "for model_name, sheets in sheet_mapping.items():\n",
    "    model = models[model_name]\n",
    "    print(f\"processing: {model_name}\")\n",
    "    \n",
    "\n",
    "    for sheet in sheets:\n",
    "        print(f\" data: {sheet}\")\n",
    "        \n",
    "        df = pd.read_excel(excel_file_path, sheet_name=sheet)\n",
    "        all_features = df.values\n",
    "        X = all_features[:, :-1] \n",
    "        y = all_features[:, -1]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name],\n",
    "                                   scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "        \n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\" Best parameters found: {grid_search.best_params_}\")\n",
    "        print(f\" Best AUC score: {grid_search.best_score_}\")\n",
    "        \n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\" Test AUC score:: {test_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBClassifier(learning_rate=0.1, n_estimators=32, max_depth=1, random_state=30),\n",
    "    \"Logistic Regression\": LogisticRegression(C=0.2, random_state=1),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=40, max_depth=2, random_state=1),\n",
    "    \"SVM\": SVC(kernel='poly', C=0.2, random_state=10, probability=True),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "}\n",
    "\n",
    "sheet_mapping = {\n",
    "    \"XGBoost\": [\"xgb_cln\", \"xgb_phy\", \"xgb_both\"],\n",
    "    \"Logistic Regression\": [\"lr_cln\", \"lr_phy\", \"lr_both\"],\n",
    "    \"Random Forest\": [\"rf_cln\", \"rf_phy\", \"rf_both\"],\n",
    "    \"SVM\": [\"svm_cln\", \"svm_phy\", \"svm_both\"],\n",
    "    \"KNN\": [\"knn_cln\", \"knn_phy\", \"knn_both\"]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "predictions = {model_name: [] for model_name in models.keys()}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    a_auc_per_sheet = []  # Store the AUC value for each sheet\n",
    "    for sheet in sheet_mapping[model_name]:\n",
    "        df = pd.read_excel('Feature screened _data.xlsx', sheet_name=sheet)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        #5-fold cross validation\n",
    "        skf = StratifiedKFold(n_splits=5, random_state=10, shuffle=True) \n",
    "        a_auc, a_acc, a_f1, a_pre = [], [], [], []\n",
    "        sheet_predictions = []\n",
    "\n",
    "        for tr_idx, test_idx in skf.split(X, y):\n",
    "            lab = y\n",
    "            model.fit(X[tr_idx], y[tr_idx])\n",
    "            y_predict = model.predict_proba(X[test_idx])[:, -1] \n",
    "            sheet_predictions.extend(y_predict)  \n",
    "            aauc = roc_auc_score(y[test_idx], y_predict)\n",
    "            f1_s = f1_score(lab[test_idx.tolist()], predict)\n",
    "            prec = precision_score(lab[test_idx.tolist()], predict)\n",
    "            aauc = roc_auc_score(lab[test_idx.tolist()], y_predict[:, -1])\n",
    "            a_auc.append(aauc)\n",
    "            a_acc.append(acc)\n",
    "            a_f1.append(f1_s)\n",
    "            a_pre.append(prec)\n",
    "        mean_auc = np.mean(a_auc)\n",
    "        a_auc_per_sheet.append(mean_auc)\n",
    "        predictions[model_name].append(np.array(sheet_predictions))  \n",
    "        results[model_name] = a_auc_per_sheet\n",
    "        print(f \"{model_name}:\")\n",
    "        \n",
    "        #Calculation of auc,acc,f1\n",
    "        print(round(np.mean(a_auc), 2), '±', round(np.std(a_auc, ddof=1), 2))\n",
    "        print(round(np.mean(a_acc), 2), '±', round(np.std(a_acc, ddof=1), 2))\n",
    "        print(round(np.mean(a_f1), 2), '±', round(np.std(a_f1, ddof=1), 2))\n",
    "        print(round(np.mean(a_pre), 2), '±', round(np.std(a_pre, ddof=1), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
